{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and modules required\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to visualize the data\n",
    "import visuals as vs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stock dataset\n",
    "data = pd.read_csv(\"stockDataset_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sample data - records\n",
    "display(data.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features : [stock_industry, stock_market_cap, stock_risk_level]\n",
    "# Target: \"opinion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total count of records\n",
    "n_total = len(data)\n",
    "\n",
    "# Count of records where opinion is bad\n",
    "n_bad = len(data[data['opinion'] == 'bad'])\n",
    "\n",
    "# Count of records where opinion is good\n",
    "n_good = len(data[data['opinion'] == 'good'])\n",
    "\n",
    "# Percentage of stocks with good opinion\n",
    "p_good = 100 * n_good / n_total\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records                        = {}\".format(n_total))\n",
    "print(\"Stocks with bad opinion to buy                 = {}\".format(n_bad))\n",
    "print(\"Stocks with good opinion to buy                = {}\".format(n_good))\n",
    "print(\"Percentage of stocks with good opinion         = {:.2f}%\".format(p_good))"
   ]
  },
  {
   "source": [
    "<h1><u>Data Preprocessing</u></h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_opinion = data['opinion']\n",
    "raw_features = data.drop(['stock_symbol', 'stock_name', 'opinion'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "features = pd.get_dummies(raw_features)\n",
    "\n",
    "# Encode the 'raw_income' data to numerical values\n",
    "opinion = raw_opinion.apply(lambda x: 1 if x == 'good' else 0)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded_features = list(features.columns)\n",
    "print(\"Total features after one-hot encoding = {}\".format(len(encoded_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of features encoded\n",
    "for feature in encoded_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting: Train data and Test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting ratio = (train : test) = (80 : 20) [X - features; Y - target variable]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, opinion, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Split details\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate naive predictor perforamance\n",
    "TP = np.sum(opinion) \n",
    "FP = opinion.count() - TP\n",
    "# As considired naive case, no negative predictions\n",
    "TN = 0 \n",
    "FN = 0 \n",
    "\n",
    "# Calculate accuracy, precision and recall\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Calculate F-score\n",
    "l_rate = 0.5\n",
    "fscore = (1 + l_rate**2) * ((precision * recall) / ((l_rate**2) * precision + recall))\n",
    "\n",
    "# Results\n",
    "print(\"-------------------------\\nNaive Predictor: \\n-------------------------\\nRecall          =  {:.3f}\\nPrecision       =  {:.3f}\\nAccuracy score  =  {:.3f}\\nF-score         =  {:.3f}\\n-------------------------\".format(recall, precision, accuracy, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing fbeta score and accuracy score from sklearn\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, Y_train, X_test, Y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - Y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - Y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data with 'sample_size'\n",
    "    start = time() \n",
    "    learner = learner.fit(X_train[:sample_size],Y_train[:sample_size])\n",
    "    end = time() \n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the test set, then get predictions on the first 300 training samples\n",
    "    start = time() \n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time()\n",
    "    \n",
    "    # Calculate the total prediction time, accuracy and F-Score on the first 300 training samples and test set\n",
    "    results['pred_time'] = end - start\n",
    "    results['acc_train'] = accuracy_score(Y_train[:300],predictions_train)        \n",
    "    results['acc_test'] = accuracy_score(Y_test,predictions_test)\n",
    "    results['f_train'] = fbeta_score(Y_train[:300],predictions_train,0.5)\n",
    "    results['f_test'] = fbeta_score(Y_test,predictions_test,0.5)\n",
    "       \n",
    "    # Mini batches\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "learner = KNN(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the batch samples into three types of batches\n",
    "samples_100 = len(Y_train)\n",
    "samples_10 = int(len(Y_train)/10)\n",
    "samples_1 = int(len(Y_train)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_predict(KNN(n_neighbors=7), len(Y_train), X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results on the learners\n",
    "results = {}\n",
    "for learner in [learner]:\n",
    "    learner_name = learner.__class__.__name__\n",
    "    results[learner_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[learner_name][i] = \\\n",
    "        train_predict(learner, samples, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the two supervised learning models chosen\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the performance numericals of the two supervised learning models chosen\n",
    "print(\"--------------------------------------------------\")\n",
    "for i in results.items():\n",
    "    print(i[0])\n",
    "    display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'100%'}))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the confusion matrix for each classifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "for i,model in enumerate([learner]):\n",
    "    results = confusion_matrix(Y_test, model.predict(X_test))   \n",
    "    print('Confusion matrix for model: {}'.format(model.__class__.__name__));\n",
    "    print(results) \n",
    "    print('Accuracy Score :',accuracy_score(Y_test, model.predict(X_test)))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}